{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graded Excersizes L4 - Piotr Bonar, Frederic Evenepoel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOB1w2Pk5mHv"
      },
      "source": [
        "### Graded Exercise 1 (6 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h3hDeZjm8TrN",
        "outputId": "8dd1a1b7-2a85-4946-ed0c-5b2a90049ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-Levenshtein in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.27.1)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-Levenshtein) (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.12.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. First, use the Gutenberg corpus from the NLTK package. Using the sents() function of the corpus, take 50 sentences as test and the rest for training. In these 50 test sentences, replace one word for a random in-vocabulary word and save the original sentence for comparison. Show a selection of 5 of these in the final report. You can also try to be a bit deliberate with your choice of random words to assess complex scenarios.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zLiOJTq_ILmh",
        "outputId": "108730ec-8672-4ef6-ebe7-5c695d620c01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /Users/macbook/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/macbook/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: <s> [ Emma by Jane Austen 1816 ] </s>\n",
            "Altered:  <s> [ Emma Bay Jane Austen 1816 ] </s>\n",
            "\n",
            "Original: <s> VOLUME I </s>\n",
            "Altered:  <s> Volum I </s>\n",
            "\n",
            "Original: <s> CHAPTER I </s>\n",
            "Altered:  <s> CHAPTERS I </s>\n",
            "\n",
            "Original: <s> Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her . </s>\n",
            "Altered:  <s> Emma Woodhouse , handsome , clever , and rich , wish a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world wish very little to distress or vex her . </s>\n",
            "\n",
            "Original: <s> She was the youngest of the two daughters of a most affectionate , indulgent father ; and had , in consequence of her sister ' s marriage , been mistress of his house from a very early period . </s>\n",
            "Altered:  <s> Shew was the youngest of the two daughters of a most affectionate , indulgent father ; and had , in consequence of her sister ' s marriage , been mistress of his house from a very early period . </s>\n",
            "\n",
            "Original: <s> Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses ; and her place had been supplied by an excellent woman as governess , who had fallen little short of a mother in affection . </s>\n",
            "Altered:  <s> Her mother had died too long ago for her ta have more than an indistinct remembrance of her caresses ; and her place had been supplied by an excellent woman as governess , who had fallen little short of a mother in affection . </s>\n",
            "\n",
            "Original: <s> Sixteen years had Miss Taylor been in Mr . Woodhouse ' s family , less as a governess than a friend , very fond of both daughters , but particularly of Emma . </s>\n",
            "Altered:  <s> Sixteen years had Miss Taylor been in Mr . Woodhouse ' s family , less as a governess than a friend , very fond ON both daughters , but particularly ON Emma . </s>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import nltk\n",
        "import Levenshtein\n",
        "from nltk.corpus import gutenberg as corpus\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences = corpus.sents()\n",
        "words = [w for w in set(corpus.words()) if w.isalpha()]\n",
        "\n",
        "train_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in sentences[200:]]\n",
        "test_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in sentences[:50]]\n",
        "\n",
        "words.extend([\"<s>\", \"</s>\"])\n",
        "\n",
        "def get_similar_word(word, vocab):\n",
        "    word_lower = word.lower()\n",
        "    similar_words = [w for w in vocab if Levenshtein.distance(word_lower, w.lower()) == 1]\n",
        "    return random.choice(similar_words) if similar_words else word\n",
        "\n",
        "altered_sentences = []\n",
        "original_sentences = []\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    sentence_words = [w for w in sentence if w.isalpha()]\n",
        "    if len(sentence_words) > 1:\n",
        "        original_sentences.append(\" \".join(sentence))\n",
        "\n",
        "        word_to_replace = random.choice(sentence_words)\n",
        "        new_word = get_similar_word(word_to_replace, words)\n",
        "\n",
        "        altered_sentence = [new_word if w == word_to_replace else w for w in sentence]\n",
        "        altered_sentences.append(\" \".join(altered_sentence))\n",
        "\n",
        "for i in range(0,7):\n",
        "    print(f\"Original: {original_sentences[i]}\")\n",
        "    print(f\"Altered:  {altered_sentences[i]}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Implement and train an ùëõ-gram language model using the NLTK package. Try using bigrams and trigrams and the variations seen in class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03UU3JmwBvcG",
        "outputId": "6fc9eb70-3c19-41b1-942c-5009c3f92595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated (MLE): <s> \" I ask you to the Pequod , I\n",
            "Generated (Laplace): <s> \" I beg your pardon Sir , come ,\n",
            "Generated (StupidBackoff): <s> \" I ask you to the Pequod , I\n"
          ]
        }
      ],
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE, Laplace, StupidBackoff\n",
        "\n",
        "n = 3\n",
        "train_data_mle, vocab_mle = padded_everygram_pipeline(n, train_sentences)\n",
        "train_data_lpc, vocab_lpc = padded_everygram_pipeline(n, train_sentences)\n",
        "train_data_sbo, vocab_sbo = padded_everygram_pipeline(n, train_sentences)\n",
        "\n",
        "lm_mle = MLE(n)\n",
        "lm_lpc = Laplace(n)\n",
        "lm_sbo = StupidBackoff(alpha=0.4, order=n)\n",
        "\n",
        "\n",
        "lm_mle.fit(train_data_mle, vocab_mle)\n",
        "lm_lpc.fit(train_data_lpc, vocab_lpc)\n",
        "lm_sbo.fit(train_data_sbo, vocab_sbo)\n",
        "\n",
        "n_words = 10\n",
        "random_seed = 42\n",
        "\n",
        "print(\"Generated (MLE):\", \" \".join(lm_mle.generate(n_words, text_seed=['<s>'], random_seed=random_seed)))\n",
        "print(\"Generated (Laplace):\", \" \".join(lm_lpc.generate(n_words, text_seed=['<s>'], random_seed=random_seed)))\n",
        "print(\"Generated (StupidBackoff):\", \" \".join(lm_sbo.generate(n_words, text_seed=['<s>'], random_seed=random_seed)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Try sampling (generating sentences) from the various language models. Which combination seems better? Aid yourself with the test sentences you left earlier that do not contain errors and evaluate the perplexity of the model. Show which model has better perplexity in the report and explain why you think it is the case providing examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Sentence: <s> She was the youngest of the two daughters of a most affectionate , indulgent father ; and had , in consequence of her sister ' s marriage , been mistress of his house from a very early period . </s>\n",
            "Perplexity (MLE): inf\n",
            "Perplexity (Laplace): 51134.00\n",
            "Perplexity (Backoff): 142.26\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_sentence = test_sentences[4]  \n",
        "padded_test_sentence = list(pad_both_ends(test_sentence, n=n))  \n",
        "test_ngrams = list(nltk.ngrams(padded_test_sentence, n=n))  \n",
        "\n",
        "mle_perplexity = lm_mle.perplexity(test_ngrams)\n",
        "laplace_perplexity = lm_laplace.perplexity(test_ngrams)\n",
        "backoff_perplexity = lm_backoff.perplexity(test_ngrams)\n",
        "\n",
        "print(f\"Test Sentence: {' '.join(test_sentence)}\")\n",
        "print(f\"Perplexity (MLE): {mle_perplexity:.2f}\")\n",
        "print(f\"Perplexity (Laplace): {laplace_perplexity:.2f}\")\n",
        "print(f\"Perplexity (Backoff): {backoff_perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. The spelling corrector we want to implement follows the Noisy Channel Model. The spelled\n",
        "sentence ùëã = ùë•1‚Ä¶ùë•ùëõ is an altered version of the intended sentence ùëä = ùë§1‚Ä¶ùë§ùëõ passing\n",
        "through a noisy channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 18183.40it/s]\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[120], line 69\u001b[0m\n\u001b[1;32m     65\u001b[0m     candidate_sentences \u001b[38;5;241m=\u001b[39m candidates(close_words, vocab, sentence)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Call the main function if this script is being run directly\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[120], line 65\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m close_words \u001b[38;5;241m=\u001b[39m compute_close_words(vocab, max_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Increased the threshold to 2\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# 2. Generate candidate sentences\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m candidate_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[120], line 32\u001b[0m, in \u001b[0;36mcandidates\u001b[0;34m(close_words, vocab, sentence)\u001b[0m\n\u001b[1;32m     30\u001b[0m sent_x \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_x \u001b[38;5;129;01min\u001b[39;00m sent_x:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m word_x \u001b[38;5;129;01min\u001b[39;00m vocab\n\u001b[1;32m     33\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [sent_x]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii, word_x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sent_x):\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from nltk.lm.api import LanguageModel\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from typing import List, Dict\n",
        "\n",
        "def levenshtein(s1: str, s2: str):\n",
        "    return edit_distance(s1, s2, substitution_cost=1, transpositions=True)\n",
        "\n",
        "# This function computes close words for a vocabulary\n",
        "def compute_close_words(vocab: List[str], max_dist: int = 1):\n",
        "    vocab = np.array(vocab)\n",
        "    word_lengths = np.array([len(w) for w in vocab])\n",
        "    dict_lengths = {}\n",
        "    for l in range(min(word_lengths), max(word_lengths)+1):\n",
        "        dict_lengths[l] = vocab[word_lengths==l]\n",
        "    min_length = min(dict_lengths.keys())\n",
        "    max_length = max(dict_lengths.keys())\n",
        "    close_words = {}\n",
        "    for word in tqdm(vocab):\n",
        "        length = len(word)\n",
        "        candidate_words = []\n",
        "        d1 = max(min_length, length - max_dist)\n",
        "        d2 = min(max_length, length + max_dist)\n",
        "        for d in range(d1, d2+1):\n",
        "            candidate_words.extend(dict_lengths[d])\n",
        "        close_words[word] = [w for w in candidate_words if levenshtein(word,w) <= max_dist]\n",
        "    return close_words\n",
        "\n",
        "# This function generates candidates for a sentence\n",
        "def candidates(close_words: Dict[str, List[str]], vocab: List[str], sentence: str):\n",
        "    sent_x = sentence.split(\" \")\n",
        "    for word_x in sent_x:\n",
        "        assert word_x in vocab\n",
        "    candidates = [sent_x]\n",
        "    for ii, word_x in enumerate(sent_x):\n",
        "        for cand_w in close_words[word_x]:\n",
        "            if cand_w != word_x:\n",
        "                cand_sent = sent_x.copy()\n",
        "                cand_sent[ii] = cand_w\n",
        "                candidates.append(cand_sent)\n",
        "    return candidates\n",
        "\n",
        "# This function computes the log prior for a sentence\n",
        "def log_prior(sentence_w: List[str], lm: LanguageModel):\n",
        "    sentence_padded = ['<s>','<s>',] + sentence_w + ['</s>']\n",
        "    num_words = len(sentence_padded)\n",
        "    log_prior_w = 0.0\n",
        "    for i in range(2, num_words-1): # omit </s> because likelihoods don't have it\n",
        "    # Uses trigrams, adapt it if you change the n\n",
        "        score = lm.logscore(sentence_padded[i], [sentence_padded[i-2], sentence_padded[i-1]])\n",
        "        prior_w += score\n",
        "    return log_prior_w\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Define a sample vocabulary (or use your own)\n",
        "    vocab = [\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\", \"sentence\", \"different\", \"word\"]\n",
        "\n",
        "    # Example sentence with potential spelling errors\n",
        "    sentence = \"this is a worrld sentence\"\n",
        "    \n",
        "    # 1. Compute close words\n",
        "    close_words = compute_close_words(vocab, max_dist=2)  # Increased the threshold to 2\n",
        "    \n",
        "    # 2. Generate candidate sentences\n",
        "    candidate_sentences = candidates(close_words, vocab, sentence)\n",
        "    \n",
        "\n",
        "# Call the main function if this script is being run directly\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 7352.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: 'worrld' is not in the vocabulary!\n",
            "No valid candidate sentences found.\n"
          ]
        }
      ],
      "source": [
        "from nltk.lm.api import LanguageModel\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.corpus import gutenberg\n",
        "from tqdm import tqdm\n",
        "\n",
        "def levenshtein(s1: str, s2: str):\n",
        "    return edit_distance(s1, s2, substitution_cost=1, transpositions=True)\n",
        "\n",
        "def compute_close_words(vocab: List[str], max_dist: int = 2):  # Increased max_dist to 2\n",
        "    close_words = {}\n",
        "    for word in tqdm(vocab):\n",
        "        close_words[word] = []\n",
        "        for vocab_word in vocab:\n",
        "            dist = levenshtein(word, vocab_word)\n",
        "            if dist <= max_dist:  # Use larger threshold\n",
        "                close_words[word].append(vocab_word)\n",
        "    return close_words\n",
        "\n",
        "def candidates(close_words: Dict[str, List[str]], vocab: List[str], sentence: str):\n",
        "    sent_x = sentence.split(\" \")\n",
        "    for word_x in sent_x:\n",
        "        if word_x not in vocab:  # Handle unknown words\n",
        "            print(f\"Warning: '{word_x}' is not in the vocabulary!\")\n",
        "            continue\n",
        "    candidate_sentences = [sent_x]\n",
        "    for ii, word_x in enumerate(sent_x):\n",
        "        if word_x in close_words:\n",
        "            for cand_w in close_words[word_x]:\n",
        "                if cand_w != word_x:\n",
        "                    cand_sent = sent_x.copy()\n",
        "                    cand_sent[ii] = cand_w\n",
        "                    candidate_sentences.append(cand_sent)\n",
        "    return candidate_sentences\n",
        "\n",
        "# This function computes the log prior for a sentence\n",
        "def log_prior(sentence_w: List[str], lm: LanguageModel):\n",
        "    sentence_padded = ['<s>', '<s>'] + sentence_w + ['</s>']\n",
        "    num_words = len(sentence_padded)\n",
        "    log_prior_w = 0.0\n",
        "    for i in range(2, num_words):  # Adjusted this loop to go till the last word\n",
        "        score = lm.logscore(sentence_padded[i], [sentence_padded[i-2], sentence_padded[i-1]])\n",
        "        log_prior_w += score\n",
        "    return log_prior_w\n",
        "\n",
        "def main():\n",
        "    # Define a sample vocabulary (or use your own)\n",
        "    vocab = [\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\", \"sentence\", \"different\", \"word\"]\n",
        "\n",
        "    # Example sentence with potential spelling errors\n",
        "    sentence = \"this is a worrld sentence\"\n",
        "    \n",
        "    # 1. Compute close words\n",
        "    close_words = compute_close_words(vocab, max_dist=2)  # Increased the threshold to 2\n",
        "    \n",
        "    # 2. Generate candidate sentences\n",
        "    candidate_sentences = candidates(close_words, vocab, sentence)\n",
        "    \n",
        "    # 3. Train a language model (e.g., using the Gutenberg corpus)\n",
        "    train_data = gutenberg.sents('austen-sense.txt')  # Sample corpus\n",
        "    train_data = [list(map(str.lower, sent)) for sent in train_data]  # Optional: convert to lowercase\n",
        "    n = 3  # Trigram model\n",
        "    train_data, padded_vocab = padded_everygram_pipeline(n, train_data)\n",
        "    \n",
        "    lm = MLE(n)  # Maximum likelihood estimation\n",
        "    lm.fit(train_data, padded_vocab)\n",
        "    \n",
        "    # 4. Compute log priors for each candidate sentence\n",
        "    best_score = float('-inf')\n",
        "    best_sentence = None\n",
        "    for candidate in candidate_sentences:\n",
        "        # Compute log prior for the candidate sentence\n",
        "        score = log_prior(candidate, lm)\n",
        "        \n",
        "        # Update best sentence if the score is better\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_sentence = candidate\n",
        "    \n",
        "    # 5. Output the best sentence\n",
        "    if best_sentence:\n",
        "        print(\"Best candidate sentence:\", ' '.join(best_sentence))\n",
        "        print(\"Log prior score:\", best_score)\n",
        "    else:\n",
        "        print(\"No valid candidate sentences found.\")\n",
        "\n",
        "# Call the main function if this script is being run directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /Users/macbook/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/macbook/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading corpus...\n",
            "Loaded 93749 sentences.\n",
            "Vocabulary size: 42209 words.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "iteration over a 0-d array",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[121], line 120\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[121], line 110\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(word \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m corpus \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vocab)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m close_words \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_close_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose words computation complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis is a tast\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "Cell \u001b[0;32mIn[121], line 16\u001b[0m, in \u001b[0;36mcompute_close_words\u001b[0;34m(vocab, max_dist)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_close_words\u001b[39m(vocab: List[\u001b[38;5;28mstr\u001b[39m], max_dist: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m     15\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(vocab)\n\u001b[0;32m---> 16\u001b[0m     word_lengths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     17\u001b[0m     dict_lengths \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(word_lengths), \u001b[38;5;28mmax\u001b[39m(word_lengths) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
            "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
          ]
        }
      ],
      "source": [
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from nltk.tokenize import word_tokenize\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Levenshtein distance function\n",
        "def levenshtein(s1: str, s2: str) -> int:\n",
        "    return edit_distance(s1, s2, substitution_cost=1, transpositions=True)\n",
        "\n",
        "# Compute close words within a Levenshtein distance\n",
        "def compute_close_words(vocab: List[str], max_dist: int = 1) -> Dict[str, List[str]]:\n",
        "    vocab = np.array(vocab)\n",
        "    word_lengths = np.array([len(w) for w in vocab])\n",
        "    dict_lengths = {}\n",
        "    for l in range(min(word_lengths), max(word_lengths) + 1):\n",
        "        dict_lengths[l] = vocab[word_lengths == l]\n",
        "    \n",
        "    min_length = min(dict_lengths.keys())\n",
        "    max_length = max(dict_lengths.keys())\n",
        "    close_words = {}\n",
        "    \n",
        "    for word in tqdm(vocab):\n",
        "        length = len(word)\n",
        "        candidate_words = []\n",
        "        d1 = max(min_length, length - max_dist)\n",
        "        d2 = min(max_length, length + max_dist)\n",
        "        \n",
        "        for d in range(d1, d2 + 1):\n",
        "            candidate_words.extend(dict_lengths[d])\n",
        "        \n",
        "        close_words[word] = [w for w in candidate_words if levenshtein(word, w) <= max_dist]\n",
        "    \n",
        "    return close_words\n",
        "\n",
        "# Generate candidate sentences based on close words\n",
        "def candidates(close_words: Dict[str, List[str]], vocab: List[str], sentence: str) -> List[List[str]]:\n",
        "    sent_x = word_tokenize(sentence)\n",
        "    candidate_list = [sent_x]\n",
        "    \n",
        "    for ii, word_x in enumerate(sent_x):\n",
        "        if word_x not in vocab:  # Only correct words not in the vocabulary\n",
        "            for cand_w in close_words.get(word_x, []):\n",
        "                if cand_w != word_x:\n",
        "                    cand_sent = sent_x.copy()\n",
        "                    cand_sent[ii] = cand_w\n",
        "                    candidate_list.append(cand_sent)\n",
        "    \n",
        "    return candidate_list\n",
        "\n",
        "# Compute the log prior probability for a sentence\n",
        "def log_prior(sentence_w: List[str], lm: MLE) -> float:\n",
        "    sentence_padded = ['<s>', '<s>'] + sentence_w + ['</s>']\n",
        "    num_words = len(sentence_padded)\n",
        "    log_prior_w = 0.0\n",
        "    \n",
        "    for i in range(2, num_words):  # Trigrams: i-2, i-1, i\n",
        "        score = lm.logscore(sentence_padded[i], [sentence_padded[i-2], sentence_padded[i-1]])\n",
        "        log_prior_w += score\n",
        "    \n",
        "    return log_prior_w\n",
        "\n",
        "# Main function to apply the spelling correction\n",
        "def noisy_channel_correction(sentence: str, close_words: Dict[str, List[str]], lm: MLE) -> str:\n",
        "    vocab = set(lm.vocab)\n",
        "    candidates_list = candidates(close_words, vocab, sentence)\n",
        "    \n",
        "    best_score = -float('inf')\n",
        "    best_candidate = sentence\n",
        "    \n",
        "    for candidate in candidates_list:\n",
        "        log_p = log_prior(candidate, lm)  # Log prior probability\n",
        "        # Calculate the likelihood (assuming substitution likelihood is 0.95 for matching words and 0.05 for others)\n",
        "        log_likelihood = sum([np.log(0.95) if word == candidate[i] else np.log(0.05) for i, word in enumerate(candidate)])\n",
        "        total_score = log_p + log_likelihood\n",
        "        \n",
        "        if total_score > best_score:\n",
        "            best_score = total_score\n",
        "            best_candidate = ' '.join(candidate)\n",
        "    \n",
        "    return best_candidate\n",
        "\n",
        "# Train a language model on the Gutenberg corpus\n",
        "def train_language_model(corpus: List[List[str]]) -> MLE:\n",
        "    # Prepare the data for training the language model (bigram model)\n",
        "    train_data, vocab = padded_everygram_pipeline(2, corpus)\n",
        "    lm = MLE(2)  # Bigram model\n",
        "    lm.fit(train_data, vocab)\n",
        "    return lm\n",
        "\n",
        "# Example usage:\n",
        "def main():\n",
        "    # Load the Gutenberg corpus\n",
        "    from nltk.corpus import gutenberg\n",
        "    nltk.download('gutenberg')\n",
        "    nltk.download('punkt')\n",
        "    \n",
        "    print(\"Loading corpus...\")\n",
        "    corpus_raw = gutenberg.sents()\n",
        "    corpus = [[word.lower() for word in sent] for sent in corpus_raw if len(sent) > 2]\n",
        "    print(f\"Loaded {len(corpus)} sentences.\")\n",
        "    \n",
        "    lm = train_language_model(corpus)\n",
        "    \n",
        "    vocab = set(word for sentence in corpus for word in sentence)\n",
        "    print(f\"Vocabulary size: {len(vocab)} words.\")\n",
        "    \n",
        "    close_words = compute_close_words(vocab, max_dist=2)\n",
        "    print(\"Close words computation complete.\")\n",
        "    \n",
        "    sentence = \"this is a tast\"\n",
        "    print(f\"Correcting sentence: '{sentence}'\")\n",
        "    corrected = noisy_channel_correction(sentence, close_words, lm)\n",
        "    print(\"Corrected:\", corrected)\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4DwYhxijILM"
      },
      "source": [
        "### Graded Exercise 2 (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "0_xtfGJ8jL6P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /Users/macbook/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/macbook/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n",
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']\n",
            "Generated (MLE):      <s> \" I cannot reason then thou and ideal ,\n",
            "Generated (Laplace):  <s> \" I cannot receive power to Sodom and David\n",
            "Generated (Backoff):  <s> \" I cannot reason then thou and ideal ,\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import gutenberg as corpus\n",
        "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
        "from nltk.lm import MLE, Laplace, StupidBackoff\n",
        "\n",
        "words = corpus.words()\n",
        "print(words[:20])\n",
        "sentences = corpus.sents()\n",
        "print(sentences[0])\n",
        "\n",
        "training_sentences = sentences[50:]\n",
        "test_sentences = sentences[:50]\n",
        "\n",
        "n = 2\n",
        "padded_sentences = [list(pad_both_ends(sent, n=n)) for sent in training_sentences]\n",
        "train_mle, vocab_mle = padded_everygram_pipeline(n, padded_sentences)\n",
        "train_lap, vocab_lap = padded_everygram_pipeline(n, padded_sentences)\n",
        "train_back, vocab_back = padded_everygram_pipeline(n, padded_sentences)\n",
        "\n",
        "lm_mle = MLE(n)\n",
        "lm_mle.fit(train_mle, vocab_mle)\n",
        "\n",
        "lm_laplace = Laplace(n)\n",
        "lm_laplace.fit(train_lap, vocab_lap)\n",
        "\n",
        "lm_backoff = StupidBackoff(alpha=0.4, order=n)\n",
        "lm_backoff.fit(train_back, vocab_back)\n",
        "\n",
        "n_words = 10\n",
        "random_seed = 42\n",
        "\n",
        "print(\"Generated (MLE):     \", \" \".join(lm_mle.generate(n_words, text_seed=['<s>'], random_seed=random_seed)))\n",
        "print(\"Generated (Laplace): \", \" \".join(lm_laplace.generate(n_words, text_seed=['<s>'], random_seed=random_seed)))\n",
        "print(\"Generated (Backoff): \", \" \".join(lm_backoff.generate(n_words, text_seed=['<s>'], random_seed=random_seed)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graded Excersize 3 (1 point)\n",
        "Suppose that you want to compute the Perplexity metric for a given Neural Network-based\n",
        "model. You want to do so on the string\n",
        "Joseph was an elderly, nay, an old man, very old, perhaps, though hale\n",
        "and sinewy.\n",
        "The model is autoregressive, meaning that it is trained to produce a new token ùë§ùë° conditioned\n",
        "on ùë§1‚Ä¶ùë§ùë°‚àí1. Explain how you would do so. How different is it from computing the same\n",
        "metric on an ùëõ-gram model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
